# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
blueprint_name: hpc-custom-big

vars:
  project_id:  solar-study-334714
  deployment_name: hpc-custom-big
  region: us-central1
  zone: us-central1-b
  new_image_family: hpc-slurm-amd-image-family

deployment_groups:
# - group: builder-env
#   modules:
#   - source: modules/network/vpc
#     kind: terraform
#     id: network1
#   - source: modules/scripts/startup-script
#     kind: terraform
#     id: scripts_for_image
#     settings:
#       runners:
#       - type: shell
#         destination: generate_hello.sh
#         content: |
#           #!/bin/sh
#           echo "Hello World" > /home/hello.txt
#     outputs: [startup_script]

# - group: packer
#   modules:
#   - source: modules/packer/custom-image
#     kind: packer
#     id: custom-image
#     settings:
#       disk_size: 20
#       source_image_project_id: [schedmd-slurm-public]
#       source_image_family: schedmd-slurm-21-08-8-hpc-centos-7
#       image_family: $(vars.new_image_family)

- group: cluster
  modules:
  - source: modules/network/pre-existing-vpc
    kind: terraform
    id: cluster-network

  - source: modules/file-system/filestore
    kind: terraform
    id: homefs
    use: [cluster-network]
    settings:
      local_mount: /home

  - source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    kind: terraform
    id: compute_partition
    use: 
    - cluster-network
    - homefs
    settings:
      partition_name: cpu
      max_node_count: 5
      instance_image:
        family: $(vars.new_image_family)
        project: $(vars.project_id)
      machine_type: c2d-standard-112
      compute_disk_size_gb: 500
      enable_placement: true  # true forces VMs to be in the region

  - source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    kind: terraform
    id: highmem_partition
    use: 
    - cluster-network
    - homefs
    settings:
      partition_name: highmem
      max_node_count: 5
      instance_image:
        family: $(vars.new_image_family)
        project: $(vars.project_id)
      machine_type: c2d-highmem-112
      compute_disk_size_gb: 1000
      enable_placement: true  # true forces VMs to be in the region

  - source: community/modules/compute/SchedMD-slurm-on-gcp-partition
    kind: terraform
    id: gpu_partition
    use:
    - cluster-network
    - homefs
    settings:
      partition_name: gpu
      max_node_count: 20
      machine_type: a2-highgpu-1g
      compute_disk_size_gb: 500
      gpu_type: nvidia-tesla-a100
      gpu_count: 1
      instance_image:
        family: $(vars.new_image_family)
        project: $(vars.project_id)

  - source: community/modules/scheduler/SchedMD-slurm-on-gcp-controller
    kind: terraform
    id: slurm_controller
    use: 
    - cluster-network
    - homefs
    - compute_partition
    - highmem_partition
    - gpu_partition
    settings:
      login_node_count: 1
      controller_machine_type: n2d-standard-4
      instance_image:
        family: $(vars.new_image_family)
        project: $(vars.project_id)
      boot_disk_size: 2000
  
  - source: community/modules/scheduler/SchedMD-slurm-on-gcp-login-node
    kind: terraform
    id: slurm_login
    use: 
    - cluster-network
    - slurm_controller
    - homefs
    settings:
      boot_disk_size: 2000
      login_machine_type: n2d-standard-4
      instance_image:
        family: $(vars.new_image_family)
        project: $(vars.project_id)
